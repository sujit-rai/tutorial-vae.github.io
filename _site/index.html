<!DOCTYPE html>
<html lang="en-US">
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>VAE Tutorial | Sujit Rai, Prateek Munjal.</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="VAE Tutorial" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Sujit Rai, Prateek Munjal." />
<meta property="og:description" content="Sujit Rai, Prateek Munjal." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="VAE Tutorial" />
<script type="application/ld+json">
{"name":"VAE Tutorial","description":"Sujit Rai, Prateek Munjal.","@type":"WebSite","url":"http://localhost:4000/","headline":"VAE Tutorial","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">VAE Tutorial</h1>
      <h2 class="project-tagline">Sujit Rai, Prateek Munjal.</h2>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <p>This is a preliminary report about the tutorial on Variational AutoEncoder. This report consists of short summary about the concepts and experiments that will be discussed in the tutorial.</p>

<h1 id="outline">Outline</h1>
<ul>
  <li>Basics of autoencoders</li>
  <li>Generative models</li>
  <li>Problem Setting</li>
  <li>Connecting Neural Networks in Probability World</li>
  <li>Introduction to Variational autoencoder</li>
  <li>Maths behind Variational autoencoder</li>
  <li><a href="code">Code Snippet for VAE</a></li>
  <li>Experiments
    <ul>
      <li>Latent Space Visualizations</li>
      <li>Visualization of cluster formation</li>
      <li>Effect of change in weightage for KL divergance during training</li>
      <li>Effect of weightage of KL divergance on disentangled representation learning</li>
      <li>Shortcoming of VAE</li>
    </ul>
  </li>
  <li>Applications of VAE</li>
</ul>

<h1 id="basics-of-autoencoders">Basics of Autoencoders</h1>
<ul>
  <li>
    <p>Autoencoders are a special type of model where we try to reconstruct the input itself.</p>
  </li>
  <li>
    <p>One may ask why do we need to reconstruct the input if we already have the data.</p>
  </li>
</ul>

<blockquote>
  <p>The idea is pretty straightford here i.e in the process of reconstructing the input we would like the Autoencoder model to learn important properties(aka <strong>features/representation</strong> in machine learning world) which are enough informative to reconstruct the input itself.</p>
</blockquote>

<p><img src="images/autoencoder.png" alt="alt text" class="center-image" />
<em>Autoencoder Model, <a href="https://becominghuman.ai/understanding-autoencoders-unsupervised-learning-technique-82fb3fbaec2">Image Source</a></em></p>

<ul>
  <li>
    <p>Now if we closely observe above model, then one can see that first Input image(X<sub>i</sub>) is transformed to a latent space representation(Z<sub>i</sub>) and then reconstructed back to input image(X<sub>i</sub>). So here we expect the Z<sub>i</sub> to extract important features.</p>
  </li>
  <li>Lets introduce some notations that will be used in upcoming sections
    <blockquote>
      <p>X ==&gt; input data <br />
N ==&gt; Number of instances in training data <br />
X<sub>i</sub> ==&gt; i<sup>th</sup> instance of input data <br />
D ==&gt; dimension of input data <br />
Z ==&gt; dimension of latent space <br />
P(X) ==&gt; Probability distribution of Input <br /></p>
    </blockquote>
  </li>
  <li>There are two variants of autoencoder :
    <ul>
      <li><em>Overcomplete Autoencoders</em>
        <blockquote>
          <p>The autoencoders where <strong>latent space dimension is more than the input dimension</strong> are called as Overcomplete Autoencoders.</p>

          <p>It seems counter-intuitive in the first reading but it has been empirically shown that provided enough regularization on model parameters we can still can learn good representations.</p>

          <p>We will not discuss this variant in detail because it is out of the scope of tutorial.</p>
        </blockquote>
      </li>
      <li><em>Undercomplete Autoencoders</em>
        <blockquote>
          <p>The autoencoders where <strong>latent space dimension is significantly less than the input dimension</strong> are called as Undercomplete Autoencoders.</p>

          <p>In this tutorial we will focus on this variant of autoencoder in detail.</p>
        </blockquote>
      </li>
    </ul>
  </li>
</ul>

<!--These are machine learning models under unsupervised learning that come with a goal to learn good representations by trying to reconstruct the input itself. Main problem of autoencoders is not let it learn a identity function which is alleviated by regularized autoencoders(eg Sparse Autoencoders). Sparse Autoencoders come with a motive of getting sparse representations in latent space which essentially means that only few neurons are active for a particular data point. This sparse constraint in latent space forces the model to learn more good representations. In regularized autoencoders, we actually misuse the meaning of regularization. By definition regularization is our prior belief on distribution of model’s parameters where as in regularized autoencoders the regularization is a prior assumed on latent space which is **not on parameters rather on data.**
-->
<h2 id="generative-models">Generative Models</h2>

<ul>
  <li>
    <p>Similar to autoencoders,a generative model also learns in unsupervised fashion.</p>
  </li>
  <li>
    <p>In contrast to Autoencoder they come with an objective of generating the data points which follow P(X).</p>
  </li>
  <li>One can question what can we leverage out of such model. So lets see some use cases :
    <blockquote>
      <p>We can generate data points following P(X) on the fly.</p>

      <p>Henceforth we need not save the dataset once we learn a generative model on it. Mathematically we learn the marginal distribution on X.</p>

      <p>Moreover one can look learning a generative model as representation learning. (An intuitive explanation for seeing it as representation learning is that if a model can produce instances similar to training data distribution then it must have learned some useful properties about the training instances too).</p>

      <p>For representation learning, any intermediate reprsentation of the generating model can be used. Empirically it has been observed that penultimate layer serves the purpose best.</p>
    </blockquote>
  </li>
  <li>Great! Now we would explore that why is it hard to achieve a generative model.
    <blockquote>
      <p>Let’s revisit the definition of Generative model i.e models which come with an objective of generating instances similar to P(X) but wait, <strong>do we even know what is P(X) ?</strong></p>

      <p>The problem is even if we have some prior knowledge on input data distribution, still approximating marginal distribution of X is intractable.(Under problem setting we will go in more detail)</p>

      <p>Therefore we would like to make use of inference models which actually tries to infer P(X) by observing X.</p>
    </blockquote>
  </li>
</ul>

<!--These type of machine learning models come with a goal to learn the true data distribution. Intuitive motivation is that if a model is able to generate plausible samples close to train data distribution then it must have learned very well representations too. These are useful where data collection is hard or next to impossible, henceforth we could use generative models to generate samples in order to augment in existing dataset. -->

<h2 id="problem-setting">Problem Setting</h2>
<p>-
		<img src="images/observation_model.png" alt="alt text" class="center-image" /></p>

<ul>
  <li>
    <p>Consider X as input data having “N” i.i.d data points.</p>
  </li>
  <li>
    <p>We assume that the input data is generated following a random process which involves a <em>hidden</em> continuous random variable <strong>z</strong>.</p>
  </li>
  <li>Process of generating data is done in two steps which are as follows :
    <blockquote>
      <p><strong>Step 1</strong> : Value z<sup>i</sup> is generated from some prior distribution p<sub>θ*</sub>(z).</p>

      <p><strong>Step 2</strong> : Then a value of x<sup>i</sup> is generated following some conditional distribution i.e p<sub>θ*</sub>(x|z).</p>

    </blockquote>
  </li>
  <li>
    <p>We assume that prior p<sub>θ*</sub>(z) and likelihood p<sub>θ*</sub>(x|z) come from parametric families of distribution p<sub>θ</sub>(z) and p<sub>θ</sub>(x|z) respectively.</p>
  </li>
  <li>
    <p>Therefore in this problem setting, many things are hidden from us like true parameters(θ*) and values z<sup>i</sup>.</p>
  </li>
  <li>Let us revisit the point that why learning marginal distribution of X is intractable.
    <ul>
      <li>
        <p>Marginal distribution can be written as follows :</p>

        <p><img src="images/marginal.png" alt="alt text" class="center-image" /></p>
      </li>
      <li>
        <p>Consider latent space to be d dimensional, then the number of integrals will be d in above equation which makes it intractable.</p>
      </li>
    </ul>
  </li>
  <li>Now we are ready to define the problems which authors of <a href="https://arxiv.org/abs/1312.6114">VAE</a> try to solve :
    <blockquote>
      <p><strong>Maximum likelihood estimation for the parameters θ</strong>.<br />One can see their use as mimicing the hidden process to generate data similar to real data.</p>

      <p><strong>Approximating posterior inference of latent variable z given a observable value x.</strong>
Such inference can be used in representation learning tasks.</p>

      <p><strong>Learning the marginal distribution over X.</strong><br />
This is useful for the applications where marginals are required, for example denoising.</p>
    </blockquote>
  </li>
</ul>

<h2 id="connecting-neural-networks-in-probability-world">Connecting Neural Networks in Probability World</h2>

<ul>
  <li>
    <p>Lets define an recognition model Q<sub>φ</sub>(z|x) which is an approximation to true posterior P<sub>θ</sub>(z|x).</p>
  </li>
  <li>
    <p>From coding theory perspective one can look the latent dimension values(z<sub>i</sub>) as latent codes or representations which are corresponding to data instances(x<sub>i</sub>).</p>
  </li>
  <li>
    <p>Since an encoder in standard autoencoder learns to map each data point(x<sub>i</sub>) to its corresponding representation(z<sub>i</sub>) in latent space while a recognition model parametrized by φ parameters learns a conditional distribution over latent codes.</p>
  </li>
  <li>
    <p>Henceforth one can easily observe that a <strong>recognition model</strong> is nothing but a <strong>probabilistic encoder</strong>.</p>
  </li>
  <li>
    <p>In a similar way, we can also say the decoder as <strong>probabilistic decoder</strong> which learns the conditional distribution P(X|Z).</p>
  </li>
</ul>

<p><img src="images/prob_encoder.png" alt="alt text" class="center-image" /></p>

<h2 id="introduction-to-variational-autoencoder">Introduction to Variational Autoencoder</h2>

<ul>
  <li>
    <p>Since decoder is a network which transforms a value z<sup>i</sup> in latent space to a value x<sup>i</sup> in input space, so the idea is to leverage the decoder in such a way that for every point in latent space it constructs something realistic in input space.</p>
  </li>
  <li>
    <p>For analogy one can see a random value in latent space as <strong>imagination</strong> and we would like the decoder to turn this imagination into something <strong>realistic</strong> in input space.</p>
  </li>
  <li>
    <p>In the variational autoencoder model, the encoder learns a conditional probability distribution i.e P(z|x) while the decoder learns a deterministic function which maps a value in latent space to a value in input space.</p>
  </li>
  <li>
    <p>Since we want to approximate intratctable true posterior distribution via the encoder’s learned distribution in latent space(which is in some arbitrary dimension), therefore the approximation should be multivariate and specifically in VAE model we assume it be a multivariate Gaussian Distribution.</p>
  </li>
  <li>
    <p>As with multivariates the covariance matrix increases quadratically with dimesions we restrict the encoder distribution family to diagonal Gaussian distributions.</p>
  </li>
  <li>
    <p>One may ask but how we actually compute the mean &amp; variance in latent space ?</p>
    <blockquote>
      <p>Instead of learning representation in latent space for a given data poin, we rather learn the mean &amp; variance of the learned distribution.</p>
    </blockquote>
  </li>
</ul>

<p><img src="images/encoder.png" alt="alt text" class="center-image" /></p>

<ul>
  <li>As the encoder &amp; decoder networks in an autoencoder are deterministic but we want our <strong>imagination</strong> to be random values in latent space, so this randomness is achieved using <strong>reparametrization trick</strong>.(We will cover about reparametrization later in this blog.)</li>
</ul>

<h2 id="intuition-behind-vae">Intuition behind VAE</h2>

<p><img src="images/intuition.png" alt="alt text" class="center-image" /></p>

<ul>
  <li>
    <p>The conditional distribution (i.e P(Z|X)) in blue color is the one which is <strong>true intractable</strong>(because we have assumed Z to be hidden in our problem setting) distribution where as the one in red color is our approximation to this intractable distribution.</p>
  </li>
  <li>
    <p>Our approximation is parametrized by parameters φ of the variational encoder network.</p>
  </li>
  <li>
    <p>Since we consider decoder as generating black box taking an input value in latent space and mapping it to some realistic value in input space.</p>
  </li>
  <li>
    <p>So one may ask how to choose random values in latent space ?</p>
    <blockquote>
      <p>Since there is decades of research available on sampling techniques so idea is to leverage the sampling from some probability distribution(say Q) as a way to generate random values in latent space.</p>

      <p>Now the values generated(in latent space) by sampling from some probability distribution should also follow the learned distribution by variational encoder else the decoder will not be able to reconstruct the images.</p>

      <p>Henceforth the learned encoder distribution should be close to the distribution(Q) from which we sample.</p>

      <p>In VAE literature we often say Q as <strong>prior distribution</strong>. We term it prior because we model our belief in form of distribution followed by the latent space.</p>

      <p>KL diveregence is the metric used to compare two probability distributions and in VAE model we use this metric to compare the learned encoder distribution and prior distribution.</p>
    </blockquote>
  </li>
</ul>

<!--Before jumping to VAE, we would like to first connect the autoencoders with probabilistic graphical models. We can view the encoder as approximating conditional probability distribution -- p(z\|x) where z is denoting latent space random var & x is input data point. Similarly decoder can be viewed as approximating a conditional distribution -- q(x\|z). So we would like to use the decoder part as generative model since it learns to map a point in latent space to a point in input space. For a decoder to get trained we also want encoder(only for training) as it provides a meaning full point in latent space corresponding to an input. But issue is that both our encoders & decoders are deterministic functions but a generative model should be stochastic (Eg : stochasticity can be seen as generating images of 3 digit but in different orientations). So to make our traditional Autoencoders stochastic we make use of re-parametrization trick which serves as core of VAE learning. -->

<h2 id="maths-behind-vae">Maths behind VAE</h2>

<p>Let’s start with the concept of information. Information carried by a sentence or a statement can be quantified by <script type="math/tex">I = -log(P(x))</script>, where <script type="math/tex">x</script> is certain event and <script type="math/tex">I</script> is information. Since, value of probability is between 0 and 1. Therefore, if the value of <script type="math/tex">P(x)</script> is close to 1 then information is close to 0 and if value of <script type="math/tex">P(x)</script> is close to 0 then information gain is very high. Therefore, an unlikely event has very high information and a likely event has very low information.</p>

<p>The average of information is known as entropy. Entropy can be calculated as <script type="math/tex">H = -\sum P(x)*log(P(x))</script>, which is just the expectation of information <script type="math/tex">I</script>.</p>

<p>Assuming there are two different distributions <script type="math/tex">P</script> and <script type="math/tex">Q</script>. Then KL Divergence is the measure of dissimilarity between <script type="math/tex">P</script> and <script type="math/tex">Q</script>. KL-Divergence is a concept which is closely related to Information and Entropy. KL-Divergence between <script type="math/tex">P</script> and <script type="math/tex">Q</script> is almost equal to Difference in entropy of <script type="math/tex">P</script> and <script type="math/tex">Q</script>. Therefore, <script type="math/tex">KL(P\|Q) \sim -\sum Q(x)*log(Q(x)) + \sum P(x)*log(P(x))</script> i.e. Difference in information obtained from distribution P and Q. Therefore, if both distributions are similar then entropy is minimum. However, KL-Divergence is calculated with respect to one distribution. Therefore, KL-Divergence of Q with respect to P can be calculated as <script type="math/tex">KL(P\|Q) = -\sum P(x)*log(Q(x)) + \sum P(x)*log(P(x))</script>. Therefore, here average information of Q is calculated with repsect to P.
The two important properties of KL-Divergence are as follows</p>
<ul>
  <li>KL-Divergence is always greater than equal to 0</li>
  <li>KL-Divergence is not symmetric i.e. <script type="math/tex">KL(P\|Q) \ne KL(Q\|P)</script></li>
</ul>

<p>Therefore, KL-Divergence is not distance measure because distance should be symmetric. KL-Divergence is a measure of dissimilarity between two distributions.</p>

<p>Since, Variational technique is a technique in inference in graphical model so, let start with graphical models. Suppose, we have <script type="math/tex">z</script> which is a hidden variable and <script type="math/tex">x</script> is an observation. We would like to compute posterior <script type="math/tex">P(z \vert x)</script>.</p>

<script type="math/tex; mode=display">P(z \vert x) = \frac{P(x \vert z) * P(z)}{P(x)} = \frac{P(x,z)}{P(x)}</script>

<p>Computing the marginal distribution <script type="math/tex">P(x)</script> is quite complicated. Because <script type="math/tex">P(x) = \int P(x \vert z)*p(z) dz</script> and this integral is intractable especially in high dimensional space. Therefore computing the marginal is one of obstacle in graphical models. Roughly there are two main approaches for calculating this integral. 1.) Monte Carlo approach (compute the integral by monte carlo integral using sampling), 2.) Variational Inference. We will use variational Inference.</p>

<p>Since, computing <script type="math/tex">P(z \vert x)</script> is not possible therefore, approximate <script type="math/tex">P(z \vert x)</script> with another distribution <script type="math/tex">Q(z)</script>. If <script type="math/tex">Q</script> is choosen to be a tractable distribution and if this tractable distribution is made close to the distribution of <script type="math/tex">P(z \vert x)</script> then the problem is solved.</p>

<p>Now, Since the problem is to make <script type="math/tex">Q(z)</script> as close as possible to <script type="math/tex">P(z \vert x)</script>. Therefore, now KL-Divergence can be used as a dissimilarity metric. If we can minimize the KL-Divergence between <script type="math/tex">P</script> and <script type="math/tex">Q</script> then we have obtained a tractable distribution <script type="math/tex">Q</script> which is similar to <script type="math/tex">P</script>.</p>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)* log\bigg(\frac{P(z \vert x)}{Q(z)}\bigg)</script>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)* log\bigg(\frac{P(x,z)}{P(x)*Q(z)}\bigg)</script>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)* log\bigg(\frac{P(x,z)}{Q(z)} * \frac{1}{P(x)}\bigg)</script>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)* \bigg[log\bigg(\frac{P(x,z)}{Q(z)}\bigg) - log(P(x))\bigg]</script>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)*log\bigg(\frac{P(x,z)}{Q(z)}\bigg) + \sum_z Q(z)*log(P(x))</script>

<p>Since summation is over z. Therefore, <script type="math/tex">log(P(x))</script> can be taken outside of summation.</p>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)*log\bigg(\frac{P(x,z)}{Q(z)}\bigg) + log(P(x)) \sum_z Q(z)</script>

<p>Since, <script type="math/tex">\sum_z Q(z) = 1</script>. Therefore,</p>

<script type="math/tex; mode=display">KL( Q(z) \| P(z \vert x)) = - \sum_z Q(z)*log\bigg(\frac{P(x,z)}{Q(z)}\bigg) + log(P(x))</script>

<script type="math/tex; mode=display">log(P(x)) = KL(Q(z) \| P(z \vert x)) + \sum_z Q(z)*log\bigg(\frac{P(x,z)}{Q(z)}\bigg)</script>

<p>Given a fixed <script type="math/tex">x</script>, <script type="math/tex">log(P(x))</script> is constant and it is independent of the distribution of <script type="math/tex">Q</script>.</p>

<p>Lets Denote <script type="math/tex">log(P(x))</script> term as <script type="math/tex">C</script>, <script type="math/tex">KL(Q(z) \| P(z \vert x))</script> term as <script type="math/tex">K</script> and <script type="math/tex">\sum_z Q(z)*log\bigg(\frac{P(x,z)}{Q(z)}\bigg)</script> term as <script type="math/tex">L</script>.</p>

<p>Now, we want to minimize <script type="math/tex">K</script>. As can be seen from equation above, <script type="math/tex">K + L</script> is equal to a constant. Therefore, instead of minimizing <script type="math/tex">K</script> we can maximize <script type="math/tex">L</script>. This <script type="math/tex">L</script> term is known as Variational Lower Bound. since <script type="math/tex">K \ge 0</script> and <script type="math/tex">C = K + L</script>. Therefore, <script type="math/tex">L \le C</script>. Hence <script type="math/tex">L</script> is lower bound of <script type="math/tex">log(P(x))</script>. In variational inference, Variational lower bound is maximized.</p>

<script type="math/tex; mode=display">L = \sum Q(z) * log\bigg( \frac{P(x,z)}{Q(z)} \bigg)</script>

<script type="math/tex; mode=display">L = \sum Q(z) * log\bigg( \frac{P(x \vert z)*P(z)}{Q(z)} \bigg)</script>

<script type="math/tex; mode=display">L = \sum Q(z) * \bigg[log(P(x \vert z)) + log\bigg(\frac{P(z)}{Q(z)}\bigg)\bigg]</script>

<script type="math/tex; mode=display">L = \sum Q(z) * log(P(x \vert z)) + \sum Q(z)*log\bigg(\frac{P(z)}{Q(z)}\bigg)</script>

<script type="math/tex; mode=display">L = \bigg(\sum Q(z) * log(P(x \vert z))\bigg) - KL(Q(z) \| P(z))</script>

<p>Here, <script type="math/tex">\sum Q(z) * log(P(x \vert z))</script> is the likelihood of observing x given hidden variable z and <script type="math/tex">KL(Q(z) \| P(z))</script> is the KL-Divergence between distributions <script type="math/tex">P</script> and <script type="math/tex">Q</script>.
Therefore, Inorder to maximize the variational lower bound <script type="math/tex">L</script>, we would like to minimize the KL-Divergence and maximize the Likelihood.</p>

<p><img src="images/img1.png" alt="alt text" class="center-image" /></p>

<p>Consider above graphical model with <script type="math/tex">z</script> as the hidden variable and <script type="math/tex">x</script> as the observation. <script type="math/tex">P(x \vert z)</script> is the mapping from hidden variable to <script type="math/tex">x</script>. Assume there exists another distribution <script type="math/tex">Q(z \vert x)</script> which maps <script type="math/tex">x</script> to <script type="math/tex">z</script>. since, <script type="math/tex">P(z \vert x)</script> is hard to compute. Therefore, we want to find another distribution <script type="math/tex">Q(z \vert x)</script> which is tractable and similar to <script type="math/tex">P(z \vert x)</script>.</p>

<p><img src="images/img2.png" alt="alt text" class="center-image" /></p>

<p>Let’s assume <script type="math/tex">Q(z \vert x)</script> is a function is obtained from a neural network which takes input <script type="math/tex">x</script> and maps it to <script type="math/tex">z</script>. Lets assume that <script type="math/tex">P(x \vert z)</script> is another neural network which takes this <script type="math/tex">z</script> and maps it to <script type="math/tex">x</script>. We will assume that <script type="math/tex">z</script> follows a gaussian distribution.</p>

<script type="math/tex; mode=display">L = \bigg(\sum Q(z) * log(P(x \vert z))\bigg) - KL(Q(z) \| P(z))</script>

<p>From the equation above we would like to maximize the L term i.e. Minimize the KL-Divergence by bringing <script type="math/tex">Q(z \vert x)</script> closer to a gaussian distribution. From <script type="math/tex">z</script> to <script type="math/tex">x'</script> there is a neural network in form of a decoder. This neural network is completely deterministic not probabilistic. Since there is a deterministic function between <script type="math/tex">z</script> and <script type="math/tex">x'</script>. Therefore, <script type="math/tex">P(z \vert x)</script> can be written as <script type="math/tex">P(x \vert x')</script>. Now, assume that the distribution <script type="math/tex">P(x \vert x')</script> follows a gaussian distribution, then there is term of the form <script type="math/tex">e^{-\vert x - x' \vert ^2}</script>. and now if we take apply on this term then it becomes of the form <script type="math/tex">\vert x - x' \vert ^ 2</script>. This term is similar to the reconstruction error or L2 loss. Similarly, if we assume that distribution of <script type="math/tex">P(x \vert x')</script> is bernoulli then the after applying log it becomes similar to cross entropy loss.</p>

<p>Now, we have a autoencoder available with cost as <script type="math/tex">\vert x - x' \vert ^ 2</script> and a KL-Divergence between <script type="math/tex">Q(z \vert x)</script> and a known distribution for example gaussian.</p>

<p>Assuming that the distribution of the <script type="math/tex">z</script> is normal. Therefore, the network can’t be trained if it produces the latent code <script type="math/tex">z</script> directly by sampling because then the gradients won’t be obtained. We would therefore like our model to produce the parameters of the normal distribution instead of the code. This parameters can then be used for obtaining the latent code. The network is therefore now supposed to produce two parameters <script type="math/tex">\mu</script> and <script type="math/tex">\sigma</script>. <script type="math/tex">\mu</script> is <script type="math/tex">d</script>-dimensional vector whereas usually <script type="math/tex">\sigma</script> is <script type="math/tex">d \times d</script>-dimensional matrix. For simplicity we would assume <script type="math/tex">\sigma</script> to be diagonal i.e. a d-dimensional vector. Therefore, the network is now trained to generate mean and variance which are <script type="math/tex">d</script>-dimensional. This mean and variance are then used to sample a code from normal distribution. The sample procedure is done a obtaining a d-dimensional random vector <script type="math/tex">n</script> and then mulitiplying it with variance and then adding the result to mean i.e. <script type="math/tex">z = \mu + (\sigma * n)</script>. Thus this will lead to constraining the network to store the information in the latent code as efficiently as possible.</p>

<h2 id="experiments">Experiments</h2>

<p>The main aim of performing experiments is to find some insights that would be helpful in understanding the intuition behind VAE. This experiments would be mainly performed on MNIST and if required some will also be performed on CIFAR10 or fashion MNIST</p>

<h4 id="the-experiments-are-as-follows">The Experiments are as follows:</h4>

<ul>
  <li>Visualization of the latent space representations
    <blockquote>
      <p>Visualizing the effect of KL Divergence on the latent space representations. T-SNE will used in order to obtain the visualization of the representation obtained by using KL Divergence and without using KL Divergence. Both the visualizations will be compared in-order to obtain useful statistics and analysis</p>
    </blockquote>
  </li>
  <li>Visualization of the cluster formation
    <blockquote>
      <p>T-SNE visualization of the representation of latent space in Normal AutoEncoder and Variational AutoEncoder will be compared in order to understand the difference in the cluster formation and the reason behind it.</p>
    </blockquote>
  </li>
  <li>Reconstruction Images</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Original Image</th>
      <th style="text-align: center">Reconstructed Image</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="images/original_new_vae.png" alt="alt text" /></td>
      <td style="text-align: center"><img src="images/reconstructed_new_vae.png" alt="alt text" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Generation of blurry samples</li>
</ul>

<p><img src="images/gen-img.png" alt="alt text" class="center-image" /></p>

<blockquote>
  <p>One of the disadvantages of VAE is that it leads to the generation of blurry samples. we except to demonstrate that effect through this experiment.</p>
</blockquote>

<ul>
  <li>Interpolation in latent space</li>
</ul>

<p><img src="images/interpolation-img.png" alt="alt text" class="center-image" /></p>

<ul>
  <li>Experiment fact on learning VAE</li>
</ul>

<p><img src="images/kl_loss.png" alt="alt text" class="center-image" /></p>

<p><img src="images/reconstruction_loss.png" alt="alt text" class="center-image" /></p>

<blockquote>
  <p>once we let increase the KL term in loss function then learning is more stable. Intuitively it means once we set a loose bound on learned distribution to match our prior, we get decoder very fast learned and then KL term comes in picture.</p>
</blockquote>

<h2 id="applications-of-vae">Applications of VAE</h2>

<h4 id="disentangled-representation-using-variational-autoencoder">Disentangled representation using variational autoencoder</h4>

<p>Variational autoencoder can also be used for learning disentangled representations. Disentangled representations are the representations in which the individual output of the neurons in the latent space are uncorrelated that is each neuron in the latent space represents some unique feature present in the input. In order to implement this class of variational autoencoder is to only add an extra hyperparameter named Beta which will act a weight on the KL divergence term of the loss function. Thus if we enforce the KL divergence term with a very high weight then this would force the network to have efficient compression of information in the latent code leading to disentangled representation.</p>

<h4 id="denoising-autoencoders">Denoising Autoencoders</h4>

<p>Autoencoders are neural networks which are commonly used for feature extraction or compression. However, if we have same or more number of nodes in the latent space then this will lead the autoencoder to not learn anything useful but to just pass the input as it is through the layers. But if we pass the input with some amount of noise and try to reconstruct the input but without the noise then this extra nodes in the latent space will learn to remove the noise from the input leading to an autoencoder which can be used for denoising the input. Therefore, this type of autoencoder is known as denoising autoencoder.</p>



      <footer class="site-footer">
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
